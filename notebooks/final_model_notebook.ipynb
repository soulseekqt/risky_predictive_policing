{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36ef3b41-9e4a-4e83-b963-78c7cf4f3dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, mean_absolute_error, classification_report, confusion_matrix\n",
    "import xgboost as xgb\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb596e6b-418b-456c-b8f4-f4b0bbbc79be",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Haversine formula to calculate distance between two lat/lon points\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = np.sin(dlat / 2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2)**2\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "    r = 6371  # Radius of Earth in kilometers\n",
    "    return r * c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4e36706-af4f-4434-b0cf-b9038a8e6909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define police districts with coordinates\n",
    "police_districts = {\n",
    "    \"District 1 (Central)\": (41.8345, -87.6216),\n",
    "    \"District 2 (Wentworth)\": (41.8027, -87.6185),\n",
    "    \"District 3 (Grand Crossing)\": (41.752, -87.6001),\n",
    "    \"District 4 (South Chicago)\": (41.7531, -87.5573),\n",
    "    \"District 5 (Calumet)\": (41.7365, -87.607),\n",
    "    \"District 6 (Gresham)\": (41.7445, -87.6616),\n",
    "    \"District 7 (Englewood)\": (41.7843, -87.6745),\n",
    "    \"District 8 (Chicago Lawn)\": (41.7794, -87.6864),\n",
    "    \"District 9 (Deering)\": (41.827, -87.667),\n",
    "    \"District 10 (Ogden)\": (41.8782, -87.7119),\n",
    "    \"District 11 (Harrison)\": (41.8589, -87.7107),\n",
    "    \"District 12 (Near West Side)\": (41.8844, -87.6456),\n",
    "    \"District 13 (Jefferson Park)\": (41.8914, -87.7377),\n",
    "    \"District 14 (Shakespeare)\": (41.8986, -87.6743),\n",
    "    \"District 15 (Austin)\": (41.8763, -87.7724),\n",
    "    \"District 16 (Albion Park)\": (41.9762, -87.7243),\n",
    "    \"District 17 (Woodlawn)\": (41.7874, -87.592),\n",
    "    \"District 18 (Pullman)\": (41.7317, -87.6079),\n",
    "    \"District 19 (Southwest)\": (41.794, -87.74),\n",
    "    \"District 20 (North Lawndale)\": (41.8655, -87.7111),\n",
    "    \"District 21 (Near North Side)\": (41.9264, -87.6482),\n",
    "    \"District 22 (Lincoln Park)\": (41.9252, -87.6549),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2879bd17-6eaf-481b-b4c2-1bf42a4df38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "file_path = '../raw_data/preprocessed_chicago.csv'  # Update with your file path\n",
    "data = pd.read_csv(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ca33458-f5cb-42ba-b306-e4775d92a7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure 'DATE OF OCCURRENCE' is in datetime format\n",
    "data['DATE'] = pd.to_datetime(data['DATE OF OCCURRENCE'])\n",
    "data['HOUR'] = data['DATE'].dt.floor('h')\n",
    "\n",
    "# Aggregate data to hourly counts\n",
    "hourly_counts = data.groupby('HOUR').size().reset_index(name='CRIME_COUNT')\n",
    "hourly_dominant_crime = data.groupby('HOUR')['OFFENSES'].agg(lambda x: x.mode()[0] if not x.mode().empty else None)\n",
    "\n",
    "hourly_data = pd.merge(hourly_counts, hourly_dominant_crime, on='HOUR', how='left')\n",
    "\n",
    "# Create cyclical features\n",
    "hourly_data['TIME_SIN'] = np.sin(2 * np.pi * hourly_data['HOUR'].dt.hour / 24)\n",
    "hourly_data['TIME_COS'] = np.cos(2 * np.pi * hourly_data['HOUR'].dt.hour / 24)\n",
    "hourly_data['MONTH_SIN'] = np.sin(2 * np.pi * hourly_data['HOUR'].dt.month / 12)\n",
    "hourly_data['MONTH_COS'] = np.cos(2 * np.pi * hourly_data['HOUR'].dt.month / 12)\n",
    "hourly_data['ROLLING_7DAY'] = hourly_data['CRIME_COUNT'].rolling(window=7).mean()\n",
    "hourly_data['DAY_OF_WEEK'] = hourly_data['HOUR'].dt.dayofweek\n",
    "hourly_data['CRIME_COUNT_LAG1'] = hourly_data['CRIME_COUNT'].shift(1)\n",
    "hourly_data['CRIME_COUNT_LAG24'] = hourly_data['CRIME_COUNT'].shift(24)\n",
    "\n",
    "# Drop NaN values\n",
    "hourly_data = hourly_data.dropna()\n",
    "\n",
    "# Add Latitude and Longitude-based features\n",
    "latitude = data['LATITUDE']\n",
    "longitude = data['LONGITUDE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0d5179b5-de64-49e8-8625-f66e0bf96b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_nearest_distance(lat, lon):\n",
    "    distances = [haversine(lat, lon, station[0], station[1]) for station in police_districts.values()]\n",
    "    return min(distances)\n",
    "\n",
    "data['DISTANCE_TO_POLICE'] = [calculate_nearest_distance(lat, lon) for lat, lon in zip(latitude, longitude)]\n",
    "\n",
    "# Merge spatial features with hourly data\n",
    "hourly_data['DISTANCE_TO_POLICE'] = data.groupby('HOUR')['DISTANCE_TO_POLICE'].transform('mean')\n",
    "\n",
    "# Add WARD feature (Label Encoding)\n",
    "label_encoder_ward = LabelEncoder() #-- label_encoder_ward.pkl\n",
    "data['WARD_ENCODED'] = label_encoder_ward.fit_transform(data['WARD'])\n",
    "\n",
    "hourly_data['WARD'] = data.groupby('HOUR')['WARD_ENCODED'].transform(lambda x: x.mode()[0] if not x.mode().empty else None)\n",
    "\n",
    "# Feature matrix and target variable\n",
    "X = hourly_data[['TIME_SIN', 'TIME_COS', 'CRIME_COUNT_LAG1', 'CRIME_COUNT_LAG24', 'ROLLING_7DAY', 'DISTANCE_TO_POLICE', 'WARD', 'DAY_OF_WEEK', 'MONTH_SIN', 'MONTH_COS']]\n",
    "\n",
    "y_reg = hourly_data['CRIME_COUNT']\n",
    "y_class = hourly_data['OFFENSES']\n",
    "\n",
    "# Encode categorical target for classification\n",
    "label_encoder = LabelEncoder()\n",
    "y_class_encoded = label_encoder.fit_transform(y_class)\n",
    "\n",
    "with open('label_encoder.pkl', 'wb') as encoder_file:\n",
    "    pickle.dump(label_encoder, encoder_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8256aa8a-d94a-46ca-9877-077f71bad81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "X_train, X_test, y_train_reg, y_test_reg = train_test_split(X, y_reg, test_size=0.3, random_state=42, shuffle=True)\n",
    "X_train_class, X_test_class, y_train_class, y_test_class = train_test_split(X, y_class_encoded, test_size=0.3, random_state=42, shuffle=True, stratify=y_class_encoded)\n",
    "\n",
    "# Handle class imbalance with SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_res, y_class_res = smote.fit_resample(X_train_class, y_train_class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "926e5161-0ead-4466-9dc0-699bb2fce4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "717fce13-8c79-407d-834b-e1b9b36fa00d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jackylui/.pyenv/versions/3.10.6/envs/risky_predictive_policing/lib/python3.10/site-packages/sklearn/model_selection/_search.py:320: UserWarning: The total space of parameters 1 is smaller than n_iter=2. Running 1 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Initialize models\n",
    "regressor = xgb.XGBRegressor(n_estimators=100, random_state=42)\n",
    "classifier = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "\n",
    "# RandomizedSearchCV for Random Forest\n",
    "param_dist_rf = {\n",
    "    'n_estimators': [200],\n",
    "    'max_depth': [20],\n",
    "    'min_samples_split': [5],\n",
    "    'min_samples_leaf': [1],\n",
    "    'bootstrap': [False]\n",
    "}\n",
    "random_search_rf = RandomizedSearchCV(\n",
    "    RandomForestClassifier(random_state=42),\n",
    "    param_distributions=param_dist_rf,\n",
    "    n_iter=2,\n",
    "    cv=3,\n",
    "    scoring='accuracy',\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "random_search_rf.fit(X_res, y_class_res)\n",
    "best_rf_classifier = random_search_rf.best_estimator_\n",
    "\n",
    "# Save the trained model\n",
    "with open('model.pkl', 'wb') as f:\n",
    "    pickle.dump(random_search_rf, f)\n",
    "\n",
    "# # Train models\n",
    "# regressor.fit(X_train, y_train_reg)\n",
    "# best_rf_classifier.fit(X_res, y_class_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c07ce9-c084-402f-a4f8-9eea26f4e914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "y_test_pred_xgb = regressor.predict(X_test)\n",
    "y_test_pred_rf = best_rf_classifier.predict(X_test_class)\n",
    "\n",
    "# Regression evaluation\n",
    "mse_test_xgb = mean_squared_error(y_test_reg, y_test_pred_xgb)\n",
    "mae_test_xgb = mean_absolute_error(y_test_reg, y_test_pred_xgb)\n",
    "r2_test_xgb = regressor.score(X_test, y_test_reg)\n",
    "\n",
    "print(f\"XGBoost Regression MSE: {mse_test_xgb}\")\n",
    "print(f\"XGBoost Regression MAE: {mae_test_xgb}\")\n",
    "print(f\"XGBoost Regression RÂ²: {r2_test_xgb}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7af5bac-d0b2-4572-81b4-df007ef56233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification evaluation\n",
    "print(\"Classification Report:\\n\", classification_report(y_test_class, y_test_pred_rf))\n",
    "\n",
    "# Evaluate using Accuracy for classification\n",
    "accuracy_test_rf = accuracy_score(y_test_class, y_test_pred_rf)\n",
    "print(f\"Random Forest Classification Accuracy on Test Set: {accuracy_test_rf}\")\n",
    "\n",
    "# Compute confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "conf_matrix = confusion_matrix(y_test_class, y_test_pred_rf)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Feature importance for Random Forest\n",
    "feature_importances = best_rf_classifier.feature_importances_\n",
    "plt.barh(X.columns, feature_importances)\n",
    "plt.xlabel(\"Feature Importance\")\n",
    "plt.title(\"Feature Importance from Random Forest\")\n",
    "plt.show()\n",
    "\n",
    "# Residuals of XGBoost Regression\n",
    "plt.scatter(y_test_reg, y_test_pred_xgb - y_test_reg)\n",
    "plt.hlines(0, xmin=y_test_reg.min(), xmax=y_test_reg.max(), colors='r', linestyles='--')\n",
    "plt.xlabel('True Values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residuals of XGBoost Regression')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6d79c4e6-60a9-4642-a998-0f7f084e2f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_input(input_data):\n",
    "\n",
    "    # Ensure 'DATE OF OCCURRENCE' is in datetime format\n",
    "    input_data['DATE'] = pd.to_datetime(input_data['DATE OF OCCURRENCE'])\n",
    "    input_data['HOUR'] = input_data['DATE'].dt.floor('h')\n",
    "\n",
    "    # Aggregate data to hourly counts\n",
    "    input_data = input_data.groupby('HOUR').size().reset_index(name='CRIME_COUNT')\n",
    "\n",
    "    #hourly_dominant_crime = data.groupby('HOUR')['OFFENSES'].agg(lambda x: x.mode()[0] if not x.mode().empty else None)\n",
    "\n",
    "    #hourly_data = pd.merge(hourly_counts, hourly_dominant_crime, on='HOUR', how='left')\n",
    "\n",
    "\n",
    "    # Convert 'DATE OF OCCURRENCE' to datetime and extract hour\n",
    "    input_data['DATE'] = pd.to_datetime(input_data['DATE OF OCCURRENCE'])\n",
    "    input_data['HOUR'] = input_data['DATE'].dt.floor('h')\n",
    "\n",
    "    # Generate cyclical features (sine and cosine of time and month)\n",
    "    input_data['TIME_SIN'] = np.sin(2 * np.pi * input_data['HOUR'].dt.hour / 24)\n",
    "    input_data['TIME_COS'] = np.cos(2 * np.pi * input_data['HOUR'].dt.hour / 24)\n",
    "    input_data['MONTH_SIN'] = np.sin(2 * np.pi * input_data['HOUR'].dt.month / 12)\n",
    "    input_data['MONTH_COS'] = np.cos(2 * np.pi * input_data['HOUR'].dt.month / 12)\n",
    "\n",
    "    # Calculate distance to nearest police district\n",
    "    input_data['DISTANCE_TO_POLICE'] = [calculate_nearest_distance(lat, lon) for lat, lon in zip(input_data['LATITUDE'], input_data['LONGITUDE'])]\n",
    "\n",
    "    # Generate lag features and rolling mean for crime counts (assuming data with past crime counts is available)\n",
    "    input_data['CRIME_COUNT_LAG1'] = input_data['CRIME_COUNT'].shift(1)\n",
    "    input_data['CRIME_COUNT_LAG24'] = input_data['CRIME_COUNT'].shift(24)\n",
    "    input_data['ROLLING_7DAY'] = input_data['CRIME_COUNT'].rolling(window=7).mean()\n",
    "\n",
    "\n",
    "    # Encode 'WARD' (you should have a predefined mapping for 'WARD' if necessary)\n",
    "    label_encoder_ward = LabelEncoder()\n",
    "    input_data['WARD_ENCODED'] = label_encoder.fit_transform(input_data['WARD'])\n",
    "\n",
    "    # Add 'DAY_OF_WEEK' feature\n",
    "    input_data['DAY_OF_WEEK'] = input_data['HOUR'].dt.dayofweek\n",
    "\n",
    "    # Drop NaN values due to lag features and rolling mean\n",
    "    input_data = input_data.dropna()\n",
    "\n",
    "    return input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f654f1f7-ea8f-4218-ac16-cf9b9ac30427",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'DATE OF OCCURRENCE'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/risky_predictive_policing/lib/python3.10/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'DATE OF OCCURRENCE'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 37\u001b[0m\n\u001b[1;32m     34\u001b[0m input_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(sample_input_data)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Call the prediction function\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m top_5_crimes \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_top_5_crimes\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Output the result\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(top_5_crimes)\n",
      "Cell \u001b[0;32mIn[42], line 4\u001b[0m, in \u001b[0;36mpredict_top_5_crimes\u001b[0;34m(input_data)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict_top_5_crimes\u001b[39m(input_data):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m# Preprocess the input data\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m     preprocessed_data \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# Extract features for prediction (same as used during training)\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     X_input \u001b[38;5;241m=\u001b[39m preprocessed_data[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTIME_SIN\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTIME_COS\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCRIME_COUNT_LAG1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCRIME_COUNT_LAG24\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mROLLING_7DAY\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      8\u001b[0m                                  \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDISTANCE_TO_POLICE\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWARD_ENCODED\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDAY_OF_WEEK\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMONTH_SIN\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMONTH_COS\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n",
      "Cell \u001b[0;32mIn[41], line 16\u001b[0m, in \u001b[0;36mpreprocess_input\u001b[0;34m(input_data)\u001b[0m\n\u001b[1;32m      8\u001b[0m input_data \u001b[38;5;241m=\u001b[39m input_data\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHOUR\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;241m.\u001b[39mreset_index(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCRIME_COUNT\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m#hourly_dominant_crime = data.groupby('HOUR')['OFFENSES'].agg(lambda x: x.mode()[0] if not x.mode().empty else None)\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m#hourly_data = pd.merge(hourly_counts, hourly_dominant_crime, on='HOUR', how='left')\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \n\u001b[1;32m     14\u001b[0m \n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Convert 'DATE OF OCCURRENCE' to datetime and extract hour\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m input_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDATE\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(\u001b[43minput_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mDATE OF OCCURRENCE\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[1;32m     17\u001b[0m input_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHOUR\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m input_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDATE\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mdt\u001b[38;5;241m.\u001b[39mfloor(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mh\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Generate cyclical features (sine and cosine of time and month)\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/risky_predictive_policing/lib/python3.10/site-packages/pandas/core/frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/risky_predictive_policing/lib/python3.10/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'DATE OF OCCURRENCE'"
     ]
    }
   ],
   "source": [
    "#Prediction function\n",
    "def predict_top_5_crimes(input_data):\n",
    "    # Preprocess the input data\n",
    "    preprocessed_data = preprocess_input(input_data)\n",
    "\n",
    "    # Extract features for prediction (same as used during training)\n",
    "    X_input = preprocessed_data[['TIME_SIN', 'TIME_COS', 'CRIME_COUNT_LAG1', 'CRIME_COUNT_LAG24', 'ROLLING_7DAY',\n",
    "                                 'DISTANCE_TO_POLICE', 'WARD_ENCODED', 'DAY_OF_WEEK', 'MONTH_SIN', 'MONTH_COS']]\n",
    "\n",
    "    # Get class probabilities from the Random Forest classifier\n",
    "    probas = classifier.predict_proba(X_input)\n",
    "\n",
    "    # Get top 5 classes with highest probabilities\n",
    "    top_5_idx = np.argsort(probas[0])[-5:][::-1]  # Sort in descending order\n",
    "    top_5_classes = label_encoder.classes_[top_5_idx]\n",
    "    top_5_probabilities = probas[0][top_5_idx]\n",
    "\n",
    "    # Create a dictionary for the top 5 predicted crimes and their probabilities\n",
    "    result = {\n",
    "        'Top 5 Crimes': {top_5_classes[i]: top_5_probabilities[i] for i in range(5)}\n",
    "    }\n",
    "\n",
    "    return result\n",
    "\n",
    "# Sample input data\n",
    "sample_input_data = {\n",
    "    'WARD': [27],\n",
    "    'DATE OF OCCURRENCE': ['1/16/2025 1:00'],\n",
    "    'LATITUDE': [41.79329893],\n",
    "    'LONGITUDE': [-87.66456619]\n",
    "}\n",
    "\n",
    "# Convert to DataFrame\n",
    "input_df = pd.DataFrame(sample_input_data)\n",
    "\n",
    "# Call the prediction function\n",
    "top_5_crimes = predict_top_5_crimes(input_df)\n",
    "\n",
    "# Output the result\n",
    "print(top_5_crimes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "dabae4a7-1241-4296-9008-2a1ef4c39aeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['WARD', 'DATE OF OCCURRENCE', 'LATITUDE', 'LONGITUDE', 'DATE', 'HOUR'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(input_df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4471f63a-3713-4210-b124-7aa3a79f785c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'WARD': [27],\n",
       " 'DATE OF OCCURRENCE': ['1/16/2025 1:00'],\n",
       " 'LATITUDE': [41.79329893],\n",
       " 'LONGITUDE': [-87.66456619]}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7ba2e433-fbed-49c8-9eea-bcaf5d08498d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LabelEncoder and Random Forest model successfully loaded!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Load the LabelEncoder\n",
    "with open(\"label_encoder_ward.pkl\", \"rb\") as file:\n",
    "    label_encoder_ward = pickle.load(file)\n",
    "\n",
    "# Load the Random Forest model\n",
    "with open(\"model.pkl\", \"rb\") as file:\n",
    "    classifier = pickle.load(file)\n",
    "\n",
    "print(\"LabelEncoder and Random Forest model successfully loaded!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d017fc9f-a861-43b9-b3e2-86f3b19a3dc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 Predicted Crimes:\n",
      "{'Top 5 Crimes': {1: 0.25625, 2: 0.21958333333333332, 0: 0.18125, 5: 0.10124999999999998, 4: 0.09583333333333334}}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Sample input data\n",
    "sample_input_data = {\n",
    "    'WARD': [4],\n",
    "    'DATE OF OCCURRENCE': ['1/16/2025 1:00'],\n",
    "    'LATITUDE': [41.79329893],\n",
    "    'LONGITUDE': [-87.66456619]\n",
    "}\n",
    "\n",
    "# Convert input data to DataFrame\n",
    "input_df = pd.DataFrame(sample_input_data)\n",
    "\n",
    "# Preprocess the input data (without encoding 'WARD')\n",
    "def preprocess_input(input_data):\n",
    "    # Ensure 'DATE OF OCCURRENCE' is in datetime format\n",
    "    input_data['DATE'] = pd.to_datetime(input_data['DATE OF OCCURRENCE'])\n",
    "    input_data['HOUR'] = input_data['DATE'].dt.floor('h')\n",
    "\n",
    "    # Generate cyclical features (sine and cosine of time and month)\n",
    "    input_data['TIME_SIN'] = np.sin(2 * np.pi * input_data['HOUR'].dt.hour / 24)\n",
    "    input_data['TIME_COS'] = np.cos(2 * np.pi * input_data['HOUR'].dt.hour / 24)\n",
    "    input_data['MONTH_SIN'] = np.sin(2 * np.pi * input_data['HOUR'].dt.month / 12)\n",
    "    input_data['MONTH_COS'] = np.cos(2 * np.pi * input_data['HOUR'].dt.month / 12)\n",
    "\n",
    "    # Don't encode 'WARD', just keep it as it is\n",
    "    # input_data['WARD_ENCODED'] = label_encoder_ward.transform(input_data['WARD'])\n",
    "\n",
    "    # Add 'DAY_OF_WEEK' feature\n",
    "    input_data['DAY_OF_WEEK'] = input_data['HOUR'].dt.dayofweek\n",
    "\n",
    "    # Add placeholder features (you can replace them with actual calculations)\n",
    "    input_data['CRIME_COUNT_LAG1'] = 0\n",
    "    input_data['CRIME_COUNT_LAG24'] = 0\n",
    "    input_data['ROLLING_7DAY'] = 0\n",
    "    input_data['DISTANCE_TO_POLICE'] = 0\n",
    "\n",
    "    return input_data\n",
    "\n",
    "# Preprocess the data\n",
    "preprocessed_input = preprocess_input(input_df)\n",
    "\n",
    "# Extract the features for prediction (including 'WARD')\n",
    "X_input = preprocessed_input[['TIME_SIN', 'TIME_COS', 'CRIME_COUNT_LAG1', 'CRIME_COUNT_LAG24', 'ROLLING_7DAY',\n",
    "                              'DISTANCE_TO_POLICE', 'WARD', 'DAY_OF_WEEK', 'MONTH_SIN', 'MONTH_COS']]\n",
    "\n",
    "# Generate predictions\n",
    "probas = classifier.predict_proba(X_input)\n",
    "\n",
    "# Get the top 5 predictions\n",
    "top_5_idx = np.argsort(probas[0])[-5:][::-1]  # Sort probabilities in descending order\n",
    "top_5_classes = classifier.classes_[top_5_idx]\n",
    "top_5_probabilities = probas[0][top_5_idx]\n",
    "\n",
    "# Display results\n",
    "result = {\n",
    "    'Top 5 Crimes': {top_5_classes[i]: top_5_probabilities[i] for i in range(5)}\n",
    "}\n",
    "print(\"Top 5 Predicted Crimes:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fe4b3d5f-5f13-4f64-aa2e-e71ee7c005be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(n_estimators=200, random_state=42)\n"
     ]
    }
   ],
   "source": [
    "print(classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0848c53-c103-4c7f-a6f5-804e8e12cd54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
